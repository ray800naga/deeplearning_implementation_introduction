{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text data\n",
    "dirs = glob('data/text/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/text\\\\dokujo-tsushin',\n",
       " 'data/text\\\\it-life-hack',\n",
       " 'data/text\\\\kaden-channel',\n",
       " 'data/text\\\\livedoor-homme',\n",
       " 'data/text\\\\movie-enter',\n",
       " 'data/text\\\\peachy',\n",
       " 'data/text\\\\smax',\n",
       " 'data/text\\\\sports-watch',\n",
       " 'data/text\\\\topic-news']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/text\\\\dokujo-tsushin\\\\dokujo-tsushin-4778030.txt',\n",
       " 'data/text\\\\dokujo-tsushin\\\\dokujo-tsushin-4778031.txt',\n",
       " 'data/text\\\\dokujo-tsushin\\\\dokujo-tsushin-4782522.txt',\n",
       " 'data/text\\\\dokujo-tsushin\\\\dokujo-tsushin-4788357.txt',\n",
       " 'data/text\\\\dokujo-tsushin\\\\dokujo-tsushin-4788362.txt',\n",
       " 'data/text\\\\dokujo-tsushin\\\\dokujo-tsushin-4788373.txt',\n",
       " 'data/text\\\\dokujo-tsushin\\\\dokujo-tsushin-4788374.txt',\n",
       " 'data/text\\\\dokujo-tsushin\\\\dokujo-tsushin-4788388.txt',\n",
       " 'data/text\\\\dokujo-tsushin\\\\dokujo-tsushin-4791665.txt',\n",
       " 'data/text\\\\dokujo-tsushin\\\\dokujo-tsushin-4796054.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get file which has '.txt'\n",
    "filepaths = glob(dirs[0] + '/*.txt')\n",
    "filepaths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/text\\\\dokujo-tsushin\\\\dokujo-tsushin-4778031.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get one file PATH\n",
    "filepath = filepaths[1]\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file\n",
    "with open(filepath, encoding='utf-8') as f:\n",
    "    text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://news.livedoor.com/article/detail/4778031/\\n',\n",
       " '2010-05-21T14:30:00+0900\\n',\n",
       " 'ネットで断ち切れない元カレとの縁\\n',\n",
       " '\\u3000携帯電話が普及する以前、恋人への連絡ツールは一般電話が普通だった。恋人と別れたら、手帳に書かれた相手の連絡先を涙ながらに消す。そうすれば、いつしか縁は切れていったものである。しかし現在は、携帯電話がありメールがあり、インターネットを開けば、ブログで相手の晩飯までわかってしまう赤裸々なご時世。切っても切れない元カレとの縁に独女たちは何を思うのであろうか？\\n',\n",
       " '\\n',\n",
       " '\\u3000「5年前に別れた彼からメールが届いてビックリしました」とは尚美さん（36歳）\\n',\n",
       " '\\n',\n",
       " '\\u3000「彼の浮気が原因で別れたのですが、現在は独り身らしいことが書いてありました。ただ、私には婚約間近の恋人がいるのでスルー。もし自分に相手がいなかったら、復活愛はあったかも。出会いの機会が少ない独女にとって、いい時代と言えるのでは？」\\n',\n",
       " '\\n',\n",
       " '\\u3000彼と交際していた当時、尚美さんは実家に住んでいた。もしもメールも携帯電話もない時代だったら、恐らく彼からの連絡はあり得なかっただろう。\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ネットで断ち切れない元カレとの縁\\n\\u3000携帯電話が普及する以前、恋人への連絡ツールは一般電話が普通だった。恋人と別れたら、手帳に書かれた相手の連絡先を涙ながらに消す。そうすれば、いつしか縁は切れていったものである。しかし現在は、携帯電話がありメールがあり、インターネットを開けば、ブログで相手の晩飯までわかってしまう赤裸々なご時世。切っても切れない元カレとの縁に独女たちは何を思うのであろうか？\\n\\n\\u3000「5年前に別れた彼からメールが届いてビックリしました」とは尚美さん（36歳）\\n\\n\\u3000「彼の浮気が原因で別れたのですが、現在は独り身らしいことが書いてありました。ただ、私には婚約間近の恋人がいるのでスルー。もし自分に相手がいなかったら、復活愛はあったかも。出会いの機会が少ない独女にとって、いい時代と言えるのでは？」\\n\\n\\u3000彼と交際していた当時、尚美さんは実家に住んでいた。もしもメールも携帯電話もない時代だったら、恐らく彼からの連絡はあり得なかっただろう。\\n\\n\\u3000一方、美加子さん（38歳）は「一般電話だけの時代のほうが、縁を切りにくかったですよ」という。\\n\\n\\u3000「今はメモリ頼りになっている分、電話番号やメールアドレスを消去してしまえばそれまでってところがありますからね。ずるずる引きずろうと思えば引きずれるし、切ろうと思えば切れる。ひと昔前は彼の電話番号を暗記していたものです。受話器を上げたり戻したり……気持ちを断つのが難しかったなぁ」\\n\\n\\u3000知恵さん（34歳）も同意見だ。\\n\\n\\u3000「情報は自分が情報網を使えば入ってくるが\\u3000自分次第でシャットアウトできるもの。だけど私は、“別れた彼と絶縁すべき”とは思っていません。心が癒されるまでの期間は連絡を断ちきり、それ以降はメールを送るなどして、友達関係に戻ることが多いですね」\\n\\n\\u3000オール・オア・ナッシングだった以前に比べ、今は縁を切る、友人に戻る、メル友関係を続けるなど、別れた後の関係性を選択できるようになった。未練が残っているうちは辛いが、ツールを上手く使えばメリットを得られることもあるだろう。ただし、落とし穴もある。最後に、律子さん（35歳）のトホホな話を紹介。\\n\\n\\u3000「10年前につきあっていた元彼とは、節目節目にメールをしています。“マイミク”で日記にコメントを書き込んだりもするし、年賀状もやりとりする仲。元彼はすでに結婚して子どもが二人いるんですね。最近はすっかり中年太りしてきてマイホームパパって感じ。いい人ではあるけど、すっかり気持ちが冷めた今となっては、なぜ自分があれほど彼に執着していたのか疑問に思えてくる。過去の恋愛の想い出をきれい残したいのなら、知らないほうがいいこともあるかもしれません」\\n\\n\\u3000恋の思い出は脳内で美化されるもの。ネットなどで友情関係を続けるのはよいが、同時に淡く切ない恋の思い出は生活感、現実感にまみれてしまうことがあるので、繋ぎすぎには注意したいところ。ま、お互い様ですけどね。（来布十和）\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract text after line 3\n",
    "with open(filepath, encoding='utf-8') as f:\n",
    "    text = f.readlines()[2:]\n",
    "    text = ''.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ネットで断ち切れない元カレとの縁携帯電話が普及する以前、恋人への連絡ツールは一般電話が普通だった。恋人と別れたら、手帳に書かれた相手の連絡先を涙ながらに消す。そうすれば、いつしか縁は切れていったものである。しかし現在は、携帯電話がありメールがあり、インターネットを開けば、ブログで相手の晩飯までわかってしまう赤裸々なご時世。切っても切れない元カレとの縁に独女たちは何を思うのであろうか？「5年前に別れた彼からメールが届いてビックリしました」とは尚美さん（36歳）「彼の浮気が原因で別れたのですが、現在は独り身らしいことが書いてありました。ただ、私には婚約間近の恋人がいるのでスルー。もし自分に相手がいなかったら、復活愛はあったかも。出会いの機会が少ない独女にとって、いい時代と言えるのでは？」彼と交際していた当時、尚美さんは実家に住んでいた。もしもメールも携帯電話もない時代だったら、恐らく彼からの連絡はあり得なかっただろう。一方、美加子さん（38歳）は「一般電話だけの時代のほうが、縁を切りにくかったですよ」という。「今はメモリ頼りになっている分、電話番号やメールアドレスを消去してしまえばそれまでってところがありますからね。ずるずる引きずろうと思えば引きずれるし、切ろうと思えば切れる。ひと昔前は彼の電話番号を暗記していたものです。受話器を上げたり戻したり……気持ちを断つのが難しかったなぁ」知恵さん（34歳）も同意見だ。「情報は自分が情報網を使えば入ってくるが自分次第でシャットアウトできるもの。だけど私は、“別れた彼と絶縁すべき”とは思っていません。心が癒されるまでの期間は連絡を断ちきり、それ以降はメールを送るなどして、友達関係に戻ることが多いですね」オール・オア・ナッシングだった以前に比べ、今は縁を切る、友人に戻る、メル友関係を続けるなど、別れた後の関係性を選択できるようになった。未練が残っているうちは辛いが、ツールを上手く使えばメリットを得られることもあるだろう。ただし、落とし穴もある。最後に、律子さん（35歳）のトホホな話を紹介。「10年前につきあっていた元彼とは、節目節目にメールをしています。“マイミク”で日記にコメントを書き込んだりもするし、年賀状もやりとりする仲。元彼はすでに結婚して子どもが二人いるんですね。最近はすっかり中年太りしてきてマイホームパパって感じ。いい人ではあるけど、すっかり気持ちが冷めた今となっては、なぜ自分があれほど彼に執着していたのか疑問に思えてくる。過去の恋愛の想い出をきれい残したいのなら、知らないほうがいいこともあるかもしれません」恋の思い出は脳内で美化されるもの。ネットなどで友情関係を続けるのはよいが、同時に淡く切ない恋の思い出は生活感、現実感にまみれてしまうことがあるので、繋ぎすぎには注意したいところ。ま、お互い様ですけどね。（来布十和）'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace escape sequence\n",
    "with open(filepath, encoding='utf-8') as f:\n",
    "    text = f.readlines()[2:]\n",
    "    text = ''.join(text)\n",
    "    text = text.replace('\\u3000', '')\n",
    "    text = text.replace('\\n', '')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functionalize\n",
    "def preprocessing(filepath):\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        text = f.readlines()[2:]\n",
    "        text = ''.join(text)\n",
    "        text = text.replace('\\u3000', '')\n",
    "        text = text.replace('\\n', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/text\\\\dokujo-tsushin\\\\dokujo-tsushin-4778031.txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = filepaths[1]\n",
    "filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'この秋は、野菜を食べて女を磨く！夏の疲れもあってか肌荒れに悩まされているミサトさん（34歳・営業）。ファンデーションのノリも良くないし、吹き出物も…。思い当たるのは…不規則な食生活と偏食。特に気になるのは野菜不足だという。「朝はトーストとコーヒーで、昼も時間がないので手軽に食べられるパン類が多いですね。夜はお弁当を買って帰ったり、パスタを作ったりします。お弁当と一緒に根野菜のキンピラやおひたしを買ったり、プチトマトを買ってみたりしますが…。野菜は全然足りていないと思うんです」（ミサトさん）。「平成20年国民健康・栄養調査」 によれば、20代、30代女性の野菜摂取の状況は、1日当たりの平均摂取量350ｇ（「健康日本21」の目標値）を大きく下回っており、他の年齢層と比べてもとても低いとのこと。ミサトさんが感じている野菜不足は、20〜30代女性の共通の悩みといえる。ミサトさん場合は、野菜は嫌いではないし、食べたく無いわけでもない。「私の食生活では、食べる機会が少ないっていうのか、野菜の食べ方がよくわからないんです」と話してくれた。鍋物が美味しい冬は、白菜や人参など野菜を食べる機会も多いだろう。しかし、ただでさえ食欲が低下する夏は、野菜を食べる機会は少なくなる。独女たちはどのように野菜を食べているのだろうか？「家で野菜を食べない分、外食で野菜を食べるようにしています」と言うのは、マユミさん（25歳・サービス業）。「大きな器でサラダを出してくれるカフェレストランがあるので、週に1〜2度行っています。野菜も10種類くらい入っているし、薄切りのフランスパンも入っているから、お腹もいっぱいになります。キャベツがおかわり自由のトンカツ屋さんに行って、思う存分キャベツを食べることもありますよ」（マユミさん）。外食も、お店選びやオーダー次第で野菜不足を補うことができるといえる。一方、家で野菜を食べる機会が多い独女は、どのような調理方法でたべているのだろうか？ 自炊派の独女にお話を伺った。「サラダはドレッシングと組み合わせがポイント」というのはヨウコさん（32歳・派遣）。以前は外食が多かったが、1年前、転職をきっかけに自炊派になった。「フレンチ、和風、ゴマ味など、ドレッシングを数種類揃えています。サラダは野菜だけだと飽きるでしょう。だから、ミックスビーンズ（缶詰）やツナ缶、コンビーフやハムを少量でも加えるようにしています。マカロニを加えたり、ゆで卵を加えたりするのもおススメですし、ポテトチップを砕いてかけたりしても結構美味しいです」（ヨウコさん）。ヨウコさんは、サラダ以外でも野菜をよく食べるとのこと。おススメ料理はキャベツの蒸し煮。「キャベツはベーコンやタマネギと一緒に蒸し煮にすると美味しいですよ。余ったらカレー風味にしたり、トマトケチャップで味を変えたりすれば飽きません」。キャベツ半玉を4等分して厚手の鍋に入れ、1カップの水とコンソメ1個を加えて、蓋をして軟らかくなるまで煮るというシンプルなもの。その時々で家にある、野菜やベーコン、ハムなどを加えればいいそうだ。タジン鍋やレンジスチーマを購入して、野菜料理を作る独女も増えている。特に電子レンジだけで手軽に調理できるシリコン製のレンジスチーマは人気が高く、コツさえ覚えれば、レパートリーもあっという間に広がるらしい。「レンジスチーマーは母に勧められました」と言うのはリサさん（33歳・デザイン関係）。「スライスしたカボチャは2〜3分でふっくらと柔らかくなりました。甘みがあって美味しかったです。ブロッコリーや人参も簡単に蒸せました。肉じゃがなども作れるらしいので、しばらくはレシピ本にそって作ってみようと思っています」（リサさん）リサさんは、夜、まとめて温野菜を作ってフレンチドレッシングであえておき、朝食で食べたり、お弁当に入れたりしている。食べ方がわかれば、野菜はぐんと身近な存在になる。トマトに含まれるリコピンには活性酸素の働きを抑える抗酸化作用がある他、クエン酸などの働きで疲労回復効果があるという。肌にいいビタミンCも多く含まれている。タマネギの辛さの成分には、抗菌作用があるので風邪の予防にも役立つし、疲労回復やストレス解消の効果もあるそうだ。積極的に食べれば肌やカラダの調子もよくなるに違いない。野菜との付き合い方を考えて、たくさん食べられるようになったなら、カラダの内側からキレイになれるだろう。また、得意料理が1つ増えるごとに、女性の自信はアップする。この秋は、野菜料理を覚えて、たくさん食べて、女を磨こう。（オフィスエムツー／神田はるひ）'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(filepaths[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "要素番号： 0\n",
      "フォルダ： data/text\\dokujo-tsushin\n",
      "------\n",
      "要素番号： 1\n",
      "フォルダ： data/text\\it-life-hack\n",
      "------\n",
      "要素番号： 2\n",
      "フォルダ： data/text\\kaden-channel\n",
      "------\n",
      "要素番号： 3\n",
      "フォルダ： data/text\\livedoor-homme\n",
      "------\n",
      "要素番号： 4\n",
      "フォルダ： data/text\\movie-enter\n",
      "------\n",
      "要素番号： 5\n",
      "フォルダ： data/text\\peachy\n",
      "------\n",
      "要素番号： 6\n",
      "フォルダ： data/text\\smax\n",
      "------\n",
      "要素番号： 7\n",
      "フォルダ： data/text\\sports-watch\n",
      "------\n",
      "要素番号： 8\n",
      "フォルダ： data/text\\topic-news\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for (label, dir) in enumerate(dirs):\n",
    "    print('要素番号：', label)\n",
    "    print('フォルダ：', dir)\n",
    "    print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, labels = [], []\n",
    "\n",
    "for (label, dir) in enumerate(dirs):\n",
    "    # get all filepaths of each dir\n",
    "    filepaths = glob('{}/*'.format(dir))\n",
    "    for filepath in filepaths:\n",
    "        # get text data\n",
    "        text = preprocessing(filepath)\n",
    "        texts.append(text)\n",
    "\n",
    "        # generate answer label\n",
    "        labels.append(label)\n",
    "\n",
    "# show data for checking\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ネットで断ち切れない元カレとの縁携帯電話が普及する以前、恋人への連絡ツールは一般電話が普通だった。恋人と別れたら、手帳に書かれた相手の連絡先を涙ながらに消す。そうすれば、いつしか縁は切れていったもので'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "mecab = MeCab.Tagger()\n",
    "\n",
    "# function for extracting nouns\n",
    "def get_nouns(text):\n",
    "    nouns = []\n",
    "    res = mecab.parse(text)\n",
    "    words = res.split('\\n')[:-2]\n",
    "    for word in words:\n",
    "        part = word.split('\\t')\n",
    "        if '名詞' in part[4]:\n",
    "            nouns.append(part[0])\n",
    "    return nouns\n",
    "\n",
    "# put extracted nouns\n",
    "word_collect = []\n",
    "for text in texts:\n",
    "    nouns = get_nouns(text)\n",
    "    word_collect.append(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7376"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show 20 samples\n",
    "len(word_collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55688"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim import corpora, matutils\n",
    "\n",
    "# check word num in dictionary\n",
    "dictionary = corpora.Dictionary(word_collect)\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6882"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering by number of occurrences\n",
    "dictionary.filter_extremes(no_below=20)\n",
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save_as_text('livedoordic.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7376, 6882), (7376,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get word num in dictionary\n",
    "n_words = len(dictionary)\n",
    "\n",
    "# generate feature vector with BoW\n",
    "x = []\n",
    "for nouns in word_collect:\n",
    "    bow_id = dictionary.doc2bow(nouns)\n",
    "    bow = matutils.corpus2dense([bow_id], n_words).T[0]\n",
    "    x.append(bow)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# transform BoW target value to numpy array\n",
    "x = np.array(x)\n",
    "t = np.array(labels)\n",
    "\n",
    "x.shape, t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check generated BoW\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6882,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# trainsform to the format which PyTorch is able to learn\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "t = torch.tensor(t, dtype=torch.int64)\n",
    "\n",
    "# transform input value and target value together to one object, dataset\n",
    "dataset = TensorDataset(x, t)\n",
    "\n",
    "# train : val : test = 60% : 20% : 20%\n",
    "n_train = int(len(dataset) * 0.6)\n",
    "n_val = int(len(dataset) * 0.2)\n",
    "n_test = int(len(dataset)) - n_train - n_val\n",
    "\n",
    "# fix random num seed for random spliting to maintain reproductivity\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# split dataset\n",
    "train, val, test = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "# batch size\n",
    "batch_size = 128\n",
    "\n",
    "# prepare DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "class Net(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(6882)\n",
    "        self.fc1 = nn.Linear(6882, 200)\n",
    "        self.fc2 = nn.Linear(200, 9)\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.val_acc = torchmetrics.Accuracy()\n",
    "        self.test_acc = torchmetrics.Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.bn(x)\n",
    "        h = self.fc1(h)\n",
    "        h = F.relu(h)\n",
    "        h = self.fc2(h)\n",
    "        return h\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, t = batch\n",
    "        y = self(x)\n",
    "        loss = F.cross_entropy(y, t)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc(y, t), on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    # processing about validation data\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, t = batch\n",
    "        y = self(x)\n",
    "        loss = F.cross_entropy(y, t)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_acc', self.val_acc(y, t), on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    # processing about test data\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, t = batch\n",
    "        y = self(x)\n",
    "        loss = F.cross_entropy(y, t)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('test_acc', self.test_acc(y, t), on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type        | Params\n",
      "------------------------------------------\n",
      "0 | bn        | BatchNorm1d | 13.8 K\n",
      "1 | fc1       | Linear      | 1.4 M \n",
      "2 | fc2       | Linear      | 1.8 K \n",
      "3 | train_acc | Accuracy    | 0     \n",
      "4 | val_acc   | Accuracy    | 0     \n",
      "5 | test_acc  | Accuracy    | 0     \n",
      "------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.569     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1894: PossibleUserWarning: The number of training batches (34) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 46/46 [00:00<00:00, 54.20it/s, loss=0.0637, v_num=32]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 46/46 [00:00<00:00, 53.07it/s, loss=0.0637, v_num=32]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:219: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 12/12 [00:00<00:00, 96.77it/s] \n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9302167892456055\n",
      "        test_loss           0.24346642196178436\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.24346642196178436, 'test_acc': 0.9302167892456055}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning\n",
    "pl.seed_everything(0)\n",
    "net = Net()\n",
    "trainer = pl.Trainer(max_epochs=30)\n",
    "trainer.fit(net, train_loader, val_loader)\n",
    "\n",
    "# test\n",
    "results = trainer.test(net, test_loader)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# generate feature vector with BoW\n",
    "n_words = len(dictionary)\n",
    "x = []\n",
    "for nouns in word_collect:\n",
    "    bow_id = dictionary.doc2bow(nouns)\n",
    "    bow = matutils.corpus2dense([bow_id], n_words).T[0]\n",
    "    x.append(bow)\n",
    "\n",
    "# instantiation\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "# specify the number of decimal places\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "tf_idf = tfidf.fit_transform(x).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 1., 1., ..., 0., 0., 0.], dtype=float32),\n",
       " array([1., 0., 0., ..., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before transoformation: BoW\n",
    "x[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0208, 0.0395, 0.0247, ..., 0.    , 0.    , 0.    ],\n",
       "       [0.029 , 0.    , 0.    , ..., 0.    , 0.    , 0.    ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after transformation: TF-IDF\n",
    "tf_idf[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.float32, torch.Tensor, torch.int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform to the format to be used in PyTorch\n",
    "x = torch.tensor(tf_idf, dtype=torch.float32)\n",
    "t = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "type(x), x.dtype, type(t), t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(x, t)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "train, val, test = torch.utils.data.random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size, shuffle=True, drop_last = True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type        | Params\n",
      "------------------------------------------\n",
      "0 | bn        | BatchNorm1d | 13.8 K\n",
      "1 | fc1       | Linear      | 1.4 M \n",
      "2 | fc2       | Linear      | 1.8 K \n",
      "3 | train_acc | Accuracy    | 0     \n",
      "4 | val_acc   | Accuracy    | 0     \n",
      "5 | test_acc  | Accuracy    | 0     \n",
      "------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.569     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:219: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1894: PossibleUserWarning: The number of training batches (34) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 46/46 [00:01<00:00, 40.58it/s, loss=0.0622, v_num=34]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 46/46 [00:01<00:00, 39.91it/s, loss=0.0622, v_num=34]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:219: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing DataLoader 0: 100%|██████████| 12/12 [00:00<00:00, 87.18it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             0.93360435962677\n",
      "        test_loss           0.22780320048332214\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.22780320048332214, 'test_acc': 0.93360435962677}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learning\n",
    "pl.seed_everything(0)\n",
    "net = Net()\n",
    "trainer = pl.Trainer(max_epochs=30)\n",
    "trainer.fit(net, train_loader, val_loader)\n",
    "results = trainer.test(net, test_loader)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'こんにちは ． 私 の 名前 は キカガク です ． \\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify '-Owakati' option\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "\n",
    "res = mecab.parse('こんにちは．私の名前はキカガクです．')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['こんにちは', '．', '私', 'の', '名前', 'は', 'キカガク', 'です', '．', '\\n']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to list with split()\n",
    "res = res.split(' ')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['こんにちは', '．', '私', 'の', '名前', 'は', 'キカガク', 'です', '．']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove escape sequence\n",
    "res = res[:-1]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['こんにちは', '．', '私', 'の', '名前', 'は', 'キカガク', 'です', '．']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return mecab.parse(text).split(' ')[:-1]\n",
    "\n",
    "# wakatigaki\n",
    "tokenize('こんにちは．私の名前はキカガクです．')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Field' from 'torchtext.data' (c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchtext\\data\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mセル37 を c:\\Users\\nagas\\OneDrive - keio.jp\\programming\\Python_practice\\deeplearning_implementation_introduction\\deeplearning_implementation_introduction\\8-6.ipynb\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nagas/OneDrive%20-%20keio.jp/programming/Python_practice/deeplearning_implementation_introduction/deeplearning_implementation_introduction/8-6.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Field, TabularDataset\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Field' from 'torchtext.data' (c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchtext\\data\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from torchtext.data import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84ad91a4a539be6a2ce1eec2640e282be04f02c4091a066051e3be5a944eca7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
