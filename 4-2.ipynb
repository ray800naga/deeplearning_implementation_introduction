{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "\n",
    "# load iris dataset\n",
    "x, t = load_iris(return_X_y=True)\n",
    "\n",
    "# transform to the format for learning with PyTorch\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "t = torch.tensor(t, dtype=torch.int64)\n",
    "\n",
    "# transform input value and target value together to a object \"dataset\"\n",
    "dataset = TensorDataset(x, t)\n",
    "\n",
    "# set sample num of each dataset\n",
    "# train : val : test = 60% : 20% : 20%\n",
    "n_train = int(len(dataset) * 0.6)\n",
    "n_val = int(len(dataset) * 0.2)\n",
    "n_test = len(dataset) - n_train - n_val\n",
    "\n",
    "# because of random split, fix random number seed to maintain reproductivity\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# split dataset\n",
    "train, val, test = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "# definition of batch size\n",
    "batch_size = 32\n",
    "\n",
    "# preparing DataLoader\n",
    "# shuffle is False in default, so only train dataset is set True\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {'name': 'x1', 'type': 'range', 'bounds': [-10., 10.]},\n",
    "    {\"name\": 'x2', 'type': 'range', 'bounds': [-10., 10.]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_function(parameters):\n",
    "    x1 = parameters.get('x1')\n",
    "    x2 = parameters.get('x2')\n",
    "    f = x1**2 + x2**2\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'ax' has no attribute 'optimize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mセル5 を /workspaces/deeplearning_implementation_introduction/4-2.ipynb\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B633a5c55736572735c6e616761735c4f6e654472697665202d206b65696f2e6a705c70726f6772616d6d696e675c507974686f6e5f70726163746963655c646565706c6561726e696e675f696d706c656d656e746174696f6e5f696e74726f64756374696f6e5c646565706c6561726e696e675f696d706c656d656e746174696f6e5f696e74726f64756374696f6e/workspaces/deeplearning_implementation_introduction/4-2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# optimize\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B633a5c55736572735c6e616761735c4f6e654472697665202d206b65696f2e6a705c70726f6772616d6d696e675c507974686f6e5f70726163746963655c646565706c6561726e696e675f696d706c656d656e746174696f6e5f696e74726f64756374696f6e5c646565706c6561726e696e675f696d706c656d656e746174696f6e5f696e74726f64756374696f6e/workspaces/deeplearning_implementation_introduction/4-2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# minimize==True: minimize, minimize==False: maximize\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B633a5c55736572735c6e616761735c4f6e654472697665202d206b65696f2e6a705c70726f6772616d6d696e675c507974686f6e5f70726163746963655c646565706c6561726e696e675f696d706c656d656e746174696f6e5f696e74726f64756374696f6e5c646565706c6561726e696e675f696d706c656d656e746174696f6e5f696e74726f64756374696f6e/workspaces/deeplearning_implementation_introduction/4-2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# because of using random number seed, fixing seed is required\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B633a5c55736572735c6e616761735c4f6e654472697665202d206b65696f2e6a705c70726f6772616d6d696e675c507974686f6e5f70726163746963655c646565706c6561726e696e675f696d706c656d656e746174696f6e5f696e74726f64756374696f6e5c646565706c6561726e696e675f696d706c656d656e746174696f6e5f696e74726f64756374696f6e/workspaces/deeplearning_implementation_introduction/4-2.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m results \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49moptimize(parameters, evaluation_function, minimize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_seed\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'ax' has no attribute 'optimize'"
     ]
    }
   ],
   "source": [
    "# optimize\n",
    "# minimize==True: minimize, minimize==False: maximize\n",
    "# because of using random number seed, fixing seed is required\n",
    "results = ax.optimize(parameters, evaluation_function, minimize=True, random_seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x1': 0.000986274809164911, 'x2': -0.13807739501201866}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check result\n",
    "best_parameters, values, experiment, model = results\n",
    "\n",
    "# parameters after optimization\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'objective': 0.02335578038955788},\n",
       " {'objective': {'objective': 0.047031639600553295}})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value of function with parameters after optimization\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Net(pl.LightningModule):\n",
    "    def __init__(self, n_mid=4, lr=0.01):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, n_mid)\n",
    "        self.fc2 = nn.Linear(n_mid, 3)\n",
    "        self.lr = lr\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.val_acc = torchmetrics.Accuracy()\n",
    "        self.test_acc = torchmetrics.Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x)\n",
    "        h = F.relu(h)\n",
    "        h = self.fc2(h)\n",
    "        return h\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, t = batch\n",
    "        y = self(x)\n",
    "        loss = F.cross_entropy(y, t)\n",
    "        self.log('train_liss', loss, on_step=True, on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc(y, t), on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    # process about val data\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, t = batch\n",
    "        y = self(x)\n",
    "        loss = F.cross_entropy(y, t)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('val_acc', self.val_acc(y, t), on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    # process about test data\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, t = batch\n",
    "        y = self(x)\n",
    "        loss = F.cross_entropy(y, t)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('test_acc', self.test_acc(y, t), on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type     | Params\n",
      "---------------------------------------\n",
      "0 | fc1       | Linear   | 20    \n",
      "1 | fc2       | Linear   | 15    \n",
      "2 | train_acc | Accuracy | 0     \n",
      "3 | val_acc   | Accuracy | 0     \n",
      "4 | test_acc  | Accuracy | 0     \n",
      "---------------------------------------\n",
      "35        Trainable params\n",
      "0         Non-trainable params\n",
      "35        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:219: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1894: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3/3 [00:00<00:00, 99.99it/s, loss=1.05, v_num=7] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 3/3 [00:00<00:00, 83.33it/s, loss=1.05, v_num=7]\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(0)\n",
    "net = Net()\n",
    "trainer = pl.Trainer(max_epochs=10)\n",
    "trainer.fit(net, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# load iris dataset\n",
    "x, t = load_iris(return_X_y=True)\n",
    "\n",
    "# transform to the format for learning with PyTorch\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "t = torch.tensor(t, dtype=torch.int64)\n",
    "\n",
    "# transform input value and target value together to a object \"dataset\"\n",
    "dataset = TensorDataset(x, t)\n",
    "\n",
    "# set sample num of each dataset\n",
    "# train : val : test = 60% : 20% : 20%\n",
    "n_train = int(len(dataset) * 0.6)\n",
    "n_val = int(len(dataset) * 0.2)\n",
    "n_test = len(dataset) - n_train - n_val\n",
    "\n",
    "# because of random split, fix random number seed to maintain reproductivity\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# split dataset\n",
    "train, val, test = random_split(dataset, [n_train, n_val, n_test])\n",
    "\n",
    "# definition of batch size\n",
    "batch_size = 32\n",
    "\n",
    "# preparing DataLoader\n",
    "# shuffle is False in default, so only train dataset is set True\n",
    "train_dataloader = torch.utils.data.DataLoader(train, batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7000)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# val_acc of last epoch\n",
    "trainer.callback_metrics['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 08-26 23:03:19] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter n_mid. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 08-26 23:03:19] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter lr. If that is not the expected value type, you can explicity specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 08-26 23:03:19] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='n_mid', parameter_type=INT, range=[1, 100]), RangeParameter(name='lr', parameter_type=FLOAT, range=[0.001, 0.1])], parameter_constraints=[]).\n",
      "[INFO 08-26 23:03:19] ax.modelbridge.dispatch_utils: Using Bayesian optimization since there are more ordered parameters than there are categories for the unordered categorical parameters.\n",
      "[INFO 08-26 23:03:19] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 5 trials, GPEI for subsequent trials]). Iterations after 5 will take longer to generate due to  model-fitting.\n",
      "[INFO 08-26 23:03:19] ax.service.managed_loop: Started full optimization with 20 steps.\n",
      "[INFO 08-26 23:03:19] ax.service.managed_loop: Running optimization trial 1...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:89: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "[ERROR 08-26 23:03:19] ax.service.managed_loop: Encountered exception during optimization: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ax\\service\\managed_loop.py\", line 226, in full_run\n",
      "    self.run_trial()\n",
      "  File \"c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ax\\utils\\common\\executils.py\", line 147, in actual_wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ax\\service\\managed_loop.py\", line 204, in run_trial\n",
      "    raw_data={\n",
      "  File \"c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ax\\service\\managed_loop.py\", line 205, in <dictcomp>\n",
      "    arm.name: self._call_evaluation_function(arm.parameters, weight)\n",
      "  File \"c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ax\\service\\managed_loop.py\", line 144, in _call_evaluation_function\n",
      "    evaluation = self.evaluation_function(parameterization)\n",
      "  File \"C:\\Users\\nagas\\AppData\\Local\\Temp\\ipykernel_16352\\3943106576.py\", line 16, in evaluation_function\n",
      "    trainer.fit(net)\n",
      "  File \"c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 696, in fit\n",
      "    self._call_and_handle_interrupt(\n",
      "  File \"c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 650, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 737, in _fit_impl\n",
      "    results = self._run(model, ckpt_path=self.ckpt_path)\n",
      "  File \"c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\", line 1093, in _run\n",
      "    verify_loop_configurations(self)\n",
      "  File \"c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py\", line 38, in verify_loop_configurations\n",
      "    __verify_train_val_loop_configuration(trainer, model)\n",
      "  File \"c:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pytorch_lightning\\trainer\\configuration_validator.py\", line 79, in __verify_train_val_loop_configuration\n",
      "    raise MisconfigurationException(\n",
      "pytorch_lightning.utilities.exceptions.MisconfigurationException: No `train_dataloader()` method defined. Lightning `Trainer` expects as minimum a `training_step()`, `train_dataloader()` and `configure_optimizers()` to be defined.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot identify best point if experiment contains no data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mセル12 を c:\\Users\\nagas\\OneDrive - keio.jp\\programming\\Python_practice\\deeplearning_implementation_introduction\\deeplearning_implementation_introduction\\4-2.ipynb\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nagas/OneDrive%20-%20keio.jp/programming/Python_practice/deeplearning_implementation_introduction/deeplearning_implementation_introduction/4-2.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m val_acc\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nagas/OneDrive%20-%20keio.jp/programming/Python_practice/deeplearning_implementation_introduction/deeplearning_implementation_introduction/4-2.ipynb#X13sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# optimization\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nagas/OneDrive%20-%20keio.jp/programming/Python_practice/deeplearning_implementation_introduction/deeplearning_implementation_introduction/4-2.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# default: minimize=False\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nagas/OneDrive%20-%20keio.jp/programming/Python_practice/deeplearning_implementation_introduction/deeplearning_implementation_introduction/4-2.ipynb#X13sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m results \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49moptimize(parameters, evaluation_function, random_seed\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nagas/OneDrive%20-%20keio.jp/programming/Python_practice/deeplearning_implementation_introduction/deeplearning_implementation_introduction/4-2.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# get result\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nagas/OneDrive%20-%20keio.jp/programming/Python_practice/deeplearning_implementation_introduction/deeplearning_implementation_introduction/4-2.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m best_parameters, values, experiment, best_model \u001b[39m=\u001b[39m results\n",
      "File \u001b[1;32mc:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ax\\service\\managed_loop.py:296\u001b[0m, in \u001b[0;36moptimize\u001b[1;34m(parameters, evaluation_function, experiment_name, objective_name, minimize, parameter_constraints, outcome_constraints, total_trials, arms_per_trial, random_seed, generation_strategy)\u001b[0m\n\u001b[0;32m    282\u001b[0m loop \u001b[39m=\u001b[39m OptimizationLoop\u001b[39m.\u001b[39mwith_evaluation_function(\n\u001b[0;32m    283\u001b[0m     parameters\u001b[39m=\u001b[39mparameters,\n\u001b[0;32m    284\u001b[0m     objective_name\u001b[39m=\u001b[39mobjective_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    293\u001b[0m     generation_strategy\u001b[39m=\u001b[39mgeneration_strategy,\n\u001b[0;32m    294\u001b[0m )\n\u001b[0;32m    295\u001b[0m loop\u001b[39m.\u001b[39mfull_run()\n\u001b[1;32m--> 296\u001b[0m parameterization, values \u001b[39m=\u001b[39m loop\u001b[39m.\u001b[39;49mget_best_point()\n\u001b[0;32m    297\u001b[0m \u001b[39mreturn\u001b[39;00m parameterization, values, loop\u001b[39m.\u001b[39mexperiment, loop\u001b[39m.\u001b[39mget_current_model()\n",
      "File \u001b[1;32mc:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ax\\service\\managed_loop.py:249\u001b[0m, in \u001b[0;36mOptimizationLoop.get_best_point\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[39mreturn\u001b[39;00m model_predictions\n\u001b[0;32m    248\u001b[0m \u001b[39m# Could not find through model, default to using raw objective.\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m parameterization, values \u001b[39m=\u001b[39m get_best_raw_objective_point(\n\u001b[0;32m    250\u001b[0m     experiment\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexperiment\n\u001b[0;32m    251\u001b[0m )\n\u001b[0;32m    252\u001b[0m \u001b[39m# For values, grab just the means to conform to TModelPredictArm format.\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    254\u001b[0m     parameterization,\n\u001b[0;32m    255\u001b[0m     (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     ),\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ax\\service\\utils\\best_point.py:118\u001b[0m, in \u001b[0;36mget_best_raw_objective_point\u001b[1;34m(experiment, optimization_config, trial_indices)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_best_raw_objective_point\u001b[39m(\n\u001b[0;32m    113\u001b[0m     experiment: Experiment,\n\u001b[0;32m    114\u001b[0m     optimization_config: Optional[OptimizationConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    115\u001b[0m     trial_indices: Optional[Iterable[\u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    116\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[TParameterization, Dict[\u001b[39mstr\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]]:\n\u001b[1;32m--> 118\u001b[0m     _, parameterization, vals \u001b[39m=\u001b[39m get_best_raw_objective_point_with_trial_index(\n\u001b[0;32m    119\u001b[0m         experiment\u001b[39m=\u001b[39;49mexperiment,\n\u001b[0;32m    120\u001b[0m         optimization_config\u001b[39m=\u001b[39;49moptimization_config,\n\u001b[0;32m    121\u001b[0m         trial_indices\u001b[39m=\u001b[39;49mtrial_indices,\n\u001b[0;32m    122\u001b[0m     )\n\u001b[0;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m parameterization, vals\n",
      "File \u001b[1;32mc:\\Users\\nagas\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ax\\service\\utils\\best_point.py:84\u001b[0m, in \u001b[0;36mget_best_raw_objective_point_with_trial_index\u001b[1;34m(experiment, optimization_config, trial_indices)\u001b[0m\n\u001b[0;32m     82\u001b[0m dat \u001b[39m=\u001b[39m experiment\u001b[39m.\u001b[39mlookup_data(trial_indices\u001b[39m=\u001b[39mtrial_indices)\n\u001b[0;32m     83\u001b[0m \u001b[39mif\u001b[39;00m dat\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mempty:\n\u001b[1;32m---> 84\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot identify best point if experiment contains no data.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m objective \u001b[39m=\u001b[39m optimization_config\u001b[39m.\u001b[39mobjective\n\u001b[0;32m     86\u001b[0m feasible_df \u001b[39m=\u001b[39m _filter_feasible_rows(\n\u001b[0;32m     87\u001b[0m     df\u001b[39m=\u001b[39mdat\u001b[39m.\u001b[39mdf,\n\u001b[0;32m     88\u001b[0m     optimization_config\u001b[39m=\u001b[39moptimization_config,\n\u001b[0;32m     89\u001b[0m     status_quo\u001b[39m=\u001b[39mexperiment\u001b[39m.\u001b[39mstatus_quo,\n\u001b[0;32m     90\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot identify best point if experiment contains no data."
     ]
    }
   ],
   "source": [
    "parameters = [\n",
    "    {'name': 'n_mid', 'type': 'range', 'bounds': [1, 100]},\\\n",
    "    {'name': 'lr', 'type': 'range', 'bounds': [0.001, 0.1]}\n",
    "]\n",
    "\n",
    "def evaluation_function(parameters):\n",
    "\n",
    "    # get parameters\n",
    "    n_mid = parameters.get('n_mid')\n",
    "    lr = parameters.get('lr')\n",
    "\n",
    "    # definition of network and learning\n",
    "    torch.manual_seed(0)\n",
    "    net = Net(n_mid=n_mid, lr=lr)\n",
    "    trainer = pl.Trainer()\n",
    "    trainer.fit(net)\n",
    "\n",
    "    # return val_acc as target function value\n",
    "    val_acc = trainer.callback_metrics['val_acc']\n",
    "\n",
    "    # calculate test_acc for checking\n",
    "    trainer.test()\n",
    "    test_acc = trainer.callback_metrics['test_acc']\n",
    "\n",
    "    # show selected hyper parameters of each trial\n",
    "    print('n_mid: ', n_mid)\n",
    "    print('lr: ', lr)\n",
    "    print('val_acc: ', val_acc)\n",
    "    print('test_acc: ', test_acc)\n",
    "\n",
    "    return val_acc\n",
    "\n",
    "# optimization\n",
    "# default: minimize=False\n",
    "results = ax.optimize(parameters, evaluation_function, random_seed=0)\n",
    "\n",
    "# get result\n",
    "best_parameters, values, experiment, best_model = results\n",
    "\n",
    "# optimized parameters\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "84ad91a4a539be6a2ce1eec2640e282be04f02c4091a066051e3be5a944eca7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
